# Project Background

A multichannel video service provider with over US$100 million in annual advertising spends, was gradually increasing its online advertising spend.
The company was extremely active in managing its substantial advertising budget, using return on investment information to make spending decisions.
Even though the internet provided a promising platform to measure ad effectiveness, doing so for display ads was proving difficult.

The two common approaches to measuring display ad effectiveness either tended to under or over value conversion rates.
To truly measure the effect on sales conversion, Star Digital needed to understand what users would have done if they had not seen campaign ads.
To do so, Star Digital designed a controlled experiment to measure the effect of display ads for one of its advertising campaigns.

This project descrives Star Digitalâ€™s experiment, focusing on the experiment design and how the company addressed various design issues.

# Project Guide

This project covers following sections:

* Introduction
* Experimental Design Overview
  - Experiment Setup
  - Group Size Considerations
  - Sampling
* Threats to causal Inference
  - Selection Bias
  - Omitted Variable Bias
  - Simultaneity Bias
  - Measurement Error
  - SUTVA Assumptions Violation
* Data Exploration and Overview
  - Importing Relevant Packages
  - Importing the data
  - Data Description
  - Missing Value Inspection
  - Data Transformation
  - Distribution Detection
  - Outlier Treatment
* Experiment Quality Check
  - Checking for validity of Randomization
  - Power of the test
* Data Analysis
  - Measuring effectiveness of ads
  - Measuring effect of ad frequency
  - Sites Comparison
* Conclusion
